{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8YnQ1N6c48jWy7uLANl7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d8ac2dcf8c14765a99dcc35db631137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dce8243e12fa496a942af4ca32990b44",
              "IPY_MODEL_5bc693946faf4477b97ef877475137c5",
              "IPY_MODEL_cf2ef7335f1d402a82ef5a49d3eed278"
            ],
            "layout": "IPY_MODEL_955dd3d716a249ec9c0d8a23f6aa342b"
          }
        },
        "dce8243e12fa496a942af4ca32990b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f012c616d3a49b79879c6557b4ad938",
            "placeholder": "​",
            "style": "IPY_MODEL_8f2b3aaf614b4d85932187514a95b0e0",
            "value": "Epoch 0:   0%"
          }
        },
        "5bc693946faf4477b97ef877475137c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_529887cab36b4127abd618c9303f183c",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_185de26c48b341d7a6a68ac605ccfed3",
            "value": 0
          }
        },
        "cf2ef7335f1d402a82ef5a49d3eed278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4038faf09644fd9047128569d2292f",
            "placeholder": "​",
            "style": "IPY_MODEL_9eb871cc22ee44ad91f19f170a906701",
            "value": " 0/23 [00:00&lt;?, ?it/s]"
          }
        },
        "955dd3d716a249ec9c0d8a23f6aa342b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "6f012c616d3a49b79879c6557b4ad938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f2b3aaf614b4d85932187514a95b0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "529887cab36b4127abd618c9303f183c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185de26c48b341d7a6a68ac605ccfed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e4038faf09644fd9047128569d2292f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb871cc22ee44ad91f19f170a906701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10udCryp7/VAD-flow/blob/main/notebooks/VAD_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "pQgT1vcwXYCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YfJ5mviMsLR",
        "outputId": "ea2c972e-a780-4c1f-fcdd-74642638cb86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:08\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda -m install textgrid\n",
        "!conda -m install lightning -c conda-forge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnAt2sbqORaD",
        "outputId": "983c6af4-18c3-46e2-d0cf-c57c02fd3574"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - textgrid\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2025.7.14  |       hbd8a1cb_0         152 KB  conda-forge\n",
            "    certifi-2025.7.14          |     pyhd8ed1ab_0         156 KB  conda-forge\n",
            "    conda-24.11.3              |  py311h38be061_0         1.1 MB  conda-forge\n",
            "    openssl-3.5.1              |       h7b32b05_0         3.0 MB  conda-forge\n",
            "    textgrid-1.5               |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         4.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  textgrid           conda-forge/noarch::textgrid-1.5-pyhd8ed1ab_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates~ --> conda-forge/noarch::ca-certificates-2025.7.14-hbd8a1cb_0 \n",
            "  certifi                           2024.12.14-pyhd8ed1ab_0 --> 2025.7.14-pyhd8ed1ab_0 \n",
            "  conda                             24.11.2-py311h38be061_1 --> 24.11.3-py311h38be061_0 \n",
            "  openssl                                  3.4.0-h7b32b05_1 --> 3.5.1-h7b32b05_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "openssl-3.5.1        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\n",
            "conda-24.11.3        | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "certifi-2025.7.14    | 156 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.5.1        | 3.0 MB    | :   1% 0.00523282961812225/1 [00:00<00:20, 20.45s/it]\n",
            "conda-24.11.3        | 1.1 MB    | :   5% 0.05457485805840216/1 [00:00<00:01,  2.01s/it]\u001b[A\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:00<00:00,  9.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:00<00:00,  9.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "certifi-2025.7.14    | 156 KB    | :  10% 0.10255704046821695/1 [00:00<00:01,  1.11s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:00<00:00,  9.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "textgrid-1.5         | 12 KB     | : 100% 1.0/1 [00:00<00:00,  8.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "textgrid-1.5         | 12 KB     | : 100% 1.0/1 [00:00<00:00,  8.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "textgrid-1.5         | 12 KB     | : 100% 1.0/1 [00:00<00:00,  8.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "conda-24.11.3        | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.01s/it]                \u001b[A\n",
            "\n",
            "certifi-2025.7.14    | 156 KB    | : 100% 1.0/1 [00:00<00:00,  1.11s/it]                \u001b[A\u001b[A\n",
            "\n",
            "openssl-3.5.1        | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.72it/s]\n",
            "conda-24.11.3        | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.50it/s]\u001b[A\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annotation Preparation"
      ],
      "metadata": {
        "id": "2-mVBgPvXb8s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfG7AXQBMiEe",
        "outputId": "32d8e573-ca68-4075-8eca-527d7f33ef77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/speech-activity-detection-datasets\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lazyrac00n/speech-activity-detection-datasets\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = path + '/Data'"
      ],
      "metadata": {
        "id": "rX9zSPDeNpzF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from textgrid import TextGrid\n",
        "\n",
        "data = []\n",
        "list_dir = []\n",
        "# loop through all files and extract annotation information\n",
        "for dirname, _, filenames in os.walk(f'{dir}/Annotation'):\n",
        "    for filename in filenames:\n",
        "        if not filename.endswith('.TextGrid'):\n",
        "            continue\n",
        "\n",
        "        tg_path = os.path.join(dirname, filename)\n",
        "        tg = TextGrid.fromFile(tg_path)\n",
        "\n",
        "        # loop through all segment in a file, each is saved as a new row with the same name\n",
        "        for interval in tg[0]:\n",
        "            data.append({\n",
        "                'name': filename.replace('.TextGrid', ''),\n",
        "                'mark': interval.mark,\n",
        "                'start': interval.minTime,\n",
        "                'end': interval.maxTime,\n",
        "                'tg_path': tg_path,\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "cbMO6woNP9Xs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['audio_path'] = df['tg_path'].str.replace('/Annotation/', '/Audio/', regex=False).str.replace('.TextGrid', '.wav', regex=False)"
      ],
      "metadata": {
        "id": "QLMt4_EVVHy_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "wQDNzA5NV6yo",
        "outputId": "10a68c6e-def6-4b32-91cc-254feda84262"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                name mark    start      end  \\\n",
              "0      mic_F02_si791    0  0.00000  2.96300   \n",
              "1      mic_F02_si791    1  2.96300  4.64300   \n",
              "2      mic_F02_si791    0  4.64300  4.84300   \n",
              "3      mic_F02_si791    1  4.84300  5.34700   \n",
              "4      mic_F02_si791    0  5.34700  6.87000   \n",
              "...              ...  ...      ...      ...   \n",
              "5705  sp27_train_sn5    0  1.15375  1.34575   \n",
              "5706  sp27_train_sn5    1  1.34575  1.84175   \n",
              "5707  sp27_train_sn5    0  1.84175  1.96975   \n",
              "5708  sp27_train_sn5    1  1.96975  2.20175   \n",
              "5709  sp27_train_sn5    0  2.20175  2.53950   \n",
              "\n",
              "                                                tg_path  \\\n",
              "0     /kaggle/input/speech-activity-detection-datase...   \n",
              "1     /kaggle/input/speech-activity-detection-datase...   \n",
              "2     /kaggle/input/speech-activity-detection-datase...   \n",
              "3     /kaggle/input/speech-activity-detection-datase...   \n",
              "4     /kaggle/input/speech-activity-detection-datase...   \n",
              "...                                                 ...   \n",
              "5705  /kaggle/input/speech-activity-detection-datase...   \n",
              "5706  /kaggle/input/speech-activity-detection-datase...   \n",
              "5707  /kaggle/input/speech-activity-detection-datase...   \n",
              "5708  /kaggle/input/speech-activity-detection-datase...   \n",
              "5709  /kaggle/input/speech-activity-detection-datase...   \n",
              "\n",
              "                                             audio_path  \n",
              "0     /kaggle/input/speech-activity-detection-datase...  \n",
              "1     /kaggle/input/speech-activity-detection-datase...  \n",
              "2     /kaggle/input/speech-activity-detection-datase...  \n",
              "3     /kaggle/input/speech-activity-detection-datase...  \n",
              "4     /kaggle/input/speech-activity-detection-datase...  \n",
              "...                                                 ...  \n",
              "5705  /kaggle/input/speech-activity-detection-datase...  \n",
              "5706  /kaggle/input/speech-activity-detection-datase...  \n",
              "5707  /kaggle/input/speech-activity-detection-datase...  \n",
              "5708  /kaggle/input/speech-activity-detection-datase...  \n",
              "5709  /kaggle/input/speech-activity-detection-datase...  \n",
              "\n",
              "[5710 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e76fd7c8-e379-495b-92a0-f4fab47df74f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>mark</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>tg_path</th>\n",
              "      <th>audio_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mic_F02_si791</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.96300</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mic_F02_si791</td>\n",
              "      <td>1</td>\n",
              "      <td>2.96300</td>\n",
              "      <td>4.64300</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mic_F02_si791</td>\n",
              "      <td>0</td>\n",
              "      <td>4.64300</td>\n",
              "      <td>4.84300</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mic_F02_si791</td>\n",
              "      <td>1</td>\n",
              "      <td>4.84300</td>\n",
              "      <td>5.34700</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mic_F02_si791</td>\n",
              "      <td>0</td>\n",
              "      <td>5.34700</td>\n",
              "      <td>6.87000</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5705</th>\n",
              "      <td>sp27_train_sn5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.15375</td>\n",
              "      <td>1.34575</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5706</th>\n",
              "      <td>sp27_train_sn5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.34575</td>\n",
              "      <td>1.84175</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5707</th>\n",
              "      <td>sp27_train_sn5</td>\n",
              "      <td>0</td>\n",
              "      <td>1.84175</td>\n",
              "      <td>1.96975</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5708</th>\n",
              "      <td>sp27_train_sn5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.96975</td>\n",
              "      <td>2.20175</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5709</th>\n",
              "      <td>sp27_train_sn5</td>\n",
              "      <td>0</td>\n",
              "      <td>2.20175</td>\n",
              "      <td>2.53950</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "      <td>/kaggle/input/speech-activity-detection-datase...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5710 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e76fd7c8-e379-495b-92a0-f4fab47df74f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e76fd7c8-e379-495b-92a0-f4fab47df74f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e76fd7c8-e379-495b-92a0-f4fab47df74f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3115c82d-4f9e-4861-bfec-dc809c0d19de\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3115c82d-4f9e-4861-bfec-dc809c0d19de')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3115c82d-4f9e-4861-bfec-dc809c0d19de button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8814243d-d399-44db-96d0-f1323f9da2ec\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8814243d-d399-44db-96d0-f1323f9da2ec button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5710,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 717,\n        \"samples\": [\n          \"SI2282\",\n          \"mic_M04_si1041\",\n          \"mic_F02_sx48\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mark\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1\",\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.9264376457612036,\n        \"min\": 0.0,\n        \"max\": 9.276,\n        \"num_unique_values\": 2820,\n        \"samples\": [\n          2.43362,\n          2.8184\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.0036901604915225,\n        \"min\": 0.12163,\n        \"max\": 11.2,\n        \"num_unique_values\": 3104,\n        \"samples\": [\n          5.28,\n          4.313\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tg_path\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 719,\n        \"samples\": [\n          \"/kaggle/input/speech-activity-detection-datasets/Data/Annotation/Female/TMIT/SI2282.TextGrid\",\n          \"/kaggle/input/speech-activity-detection-datasets/Data/Annotation/Female/TMIT/SX120.TextGrid\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_path\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 719,\n        \"samples\": [\n          \"/kaggle/input/speech-activity-detection-datasets/Data/Audio/Female/TMIT/SI2282.wav\",\n          \"/kaggle/input/speech-activity-detection-datasets/Data/Audio/Female/TMIT/SX120.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing"
      ],
      "metadata": {
        "id": "3uCEcHBDXjeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# audio framing: Test code for audio framing\n",
        "import librosa\n",
        "import librosa.util\n",
        "\n",
        "audio_sample_dir = df['audio_path'][0]\n",
        "audio, sr = librosa.load(audio_sample_dir, sr=16000)\n",
        "\n",
        "frame_dur = 0.025\n",
        "frame_length = int(frame_dur * sr)\n",
        "\n",
        "hop_dur = 0.01\n",
        "hop_length = int(hop_dur * sr)\n",
        "frames = librosa.util.frame(audio, frame_length=frame_length, hop_length=hop_length)\n",
        "frames = frames.T\n"
      ],
      "metadata": {
        "id": "-_Uccf-F4HK1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# duration: see the histogram of duration -> for deciding padding strategy\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "duration_list = []\n",
        "for audio_path in df['audio_path']:\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    duration = librosa.get_duration(y=audio, sr=sr)\n",
        "    duration_list.append(duration)\n",
        "# add duration col\n",
        "df['duration'] = duration_list"
      ],
      "metadata": {
        "id": "Wdk5tDy4xKVE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(duration_list, bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Histogram of Audio Durations\")\n",
        "plt.xlabel(\"Duration (seconds)\")\n",
        "plt.ylabel(\"Number of audio files\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uh0b9D_rFuKo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "828ceab2-3b5b-4cda-8bcf-ddf2fc8694b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['audio_path'][[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYROH4cUiHgF",
        "outputId": "fb691d7b-0884-4b8f-e90c-5b295fc9e22d"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    /kaggle/input/speech-activity-detection-datase...\n",
            "Name: audio_path, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "from textgrid import TextGrid\n",
        "import librosa\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def download_data() -> str:\n",
        "    '''\n",
        "    Download dataset from kagglehub: speech-activity-detection-datasets\n",
        "\n",
        "    Returns:\n",
        "        str: path to dataset files\n",
        "\n",
        "    Raises:\n",
        "        Exception: if error occurs\n",
        "    '''\n",
        "    try:\n",
        "      path = kagglehub.dataset_download(\"lazyrac00n/speech-activity-detection-datasets\")\n",
        "    except Exception as e:\n",
        "      print(f'Error: {e}')\n",
        "      return None\n",
        "\n",
        "    return path\n",
        "\n",
        "def build_dataframe(data_dir: str) -> pd.DataFrame:\n",
        "    '''\n",
        "    Build dataframe from dataset files.\n",
        "    The strategy is treat each segment of a audio as a new row in the dataframe.\n",
        "    Use this dataframe by get all row with the same audio name.\n",
        "\n",
        "    Each row in the dataframe has the following columns:\n",
        "    - name: name of the audio file\n",
        "    - mark: label of the segment\n",
        "    - start: start time of the segment\n",
        "    - end: end time of the segment\n",
        "    - tg_path: path to the textgrid file\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): path to dataset files\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: dataframe with columns: name, mark, start, end, tg_path, audio_path, duration\n",
        "    '''\n",
        "\n",
        "    # processing speed may still can be improved\n",
        "    data = []\n",
        "    list_dir = []\n",
        "\n",
        "    # loop through all files for processing\n",
        "    for dirname, _, filenames in os.walk(f'{data_dir}/Annotation'):\n",
        "        for filename in filenames:\n",
        "            # only open TextGrid file\n",
        "            if not filename.endswith('.TextGrid'):\n",
        "                continue\n",
        "\n",
        "            # open TextGrid for processing\n",
        "            tg_path = os.path.join(dirname, filename)\n",
        "            tg = TextGrid.fromFile(tg_path)\n",
        "\n",
        "            # loop through all interval (segment) in a file\n",
        "            for interval in tg[0]:\n",
        "                data.append({\n",
        "                    'name': filename.replace('.TextGrid', ''),\n",
        "                    'mark': interval.mark,\n",
        "                    'start': interval.minTime,\n",
        "                    'end': interval.maxTime,\n",
        "                    'tg_path': tg_path,\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    # create path for easier access\n",
        "    df['audio_path'] = df['tg_path'].str.replace('/Annotation/', '/Audio/', regex=False).str.replace('.TextGrid', '.wav', regex=False)\n",
        "    # create duration for analysis (and may be useful later)\n",
        "    df['duration'] = df['audio_path'].apply(lambda x: librosa.get_duration(filename=x))\n",
        "    return df\n",
        "\n",
        "def padding(audio, sr, max_length):\n",
        "  '''\n",
        "  padding in the end of each audio so that they have the same length.\n",
        "  padding before framing.\n",
        "  maybe can upgrade by using other mode.\n",
        "\n",
        "  Args:\n",
        "    audio: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    sr: sampling rate, it should be converted to the same in each audio (this is processed in transform())\n",
        "    max_length: the fix_length for all audio file (in second)\n",
        "  '''\n",
        "\n",
        "  if not isinstance(audio, np.ndarray):\n",
        "    audio = np.array(audio)\n",
        "  audio = librosa.util.fix_length(audio, size = max_length * sr)\n",
        "  return audio\n",
        "\n",
        "\n",
        "def get_frames(audio, sr, frame_dur = 0.025, hop_dur = 0.01) -> torch.Tensor:\n",
        "  '''\n",
        "  Audio framing (useful for Voice Activity Detection)\n",
        "  Args:\n",
        "    audio: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    sr: sampling rate, it should be converted to the same in each audio (this is processed in transform\n",
        "\n",
        "    frame_dur: frame duration (in second) that used for compute frame_length (used for framing uiwht librosa.util.frame)\n",
        "    hop_dur: hop duration (in second) that used for compute hop_length (used for framing uiwht librosa.util.frame)\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: frames with shape (num_frame, frame_size)\n",
        "  '''\n",
        "  frame_dur = frame_dur\n",
        "  frame_length = int(frame_dur * sr)\n",
        "\n",
        "  hop_dur = hop_dur\n",
        "  hop_length = int(hop_dur * sr)\n",
        "\n",
        "  if not isinstance(audio, np.ndarray):\n",
        "    audio = np.array(audio)\n",
        "\n",
        "  frames = librosa.util.frame(audio, frame_length=frame_length, hop_length=hop_length)\n",
        "\n",
        "  # transpose for easier processing (num_frame, frame_size)\n",
        "  frames = frames.T\n",
        "\n",
        "  # convert to tensor\n",
        "  if not isinstance(frames, torch.Tensor):\n",
        "    frames = torch.from_numpy(frames)\n",
        "\n",
        "  return frames # (num_frame, frame_size)\n",
        "\n",
        "def resample(audio, orig_sr, target_sr):\n",
        "  '''\n",
        "  resampling so that all audio file have the same sampling rate\n",
        "\n",
        "  Args:\n",
        "    audio: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    orig_sr: original sampling rate\n",
        "    target_sr: target sampling rate\n",
        "\n",
        "  Returns:\n",
        "    np.ndarray: resampled audio\n",
        "  '''\n",
        "  if not isinstance(audio, np.ndarray):\n",
        "    audio = np.array(audio)\n",
        "\n",
        "  audio = librosa.resample(y = audio, orig_sr = orig_sr, target_sr = target_sr)\n",
        "  return audio\n",
        "\n",
        "def transform(df, audio, orig_sr, target_sr):\n",
        "  '''\n",
        "  TODO: add mel-spectrogram option\n",
        "\n",
        "\n",
        "  transform for training, with three steps:\n",
        "  1. resampling\n",
        "  2. padding\n",
        "  3. framing\n",
        "\n",
        "  Args:\n",
        "    audio: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    orig_sr: original sampling rate\n",
        "    target_sr: target sampling rate\n",
        "\n",
        "  Returns:\n",
        "    audio: resampled audio with padding and resampling\n",
        "    frames: waveform after framing, will be converted to np.ndarray so that it can be processed with librosa, with shape (num_frame, frame_size, 1)\n",
        "  '''\n",
        "  # get max length (plus 0.5) for better number rounding\n",
        "\n",
        "  max_length = round(max(df['duration']) + 0.5)\n",
        "  # resampling: return np.ndarray\n",
        "  audio = resample(audio = audio, orig_sr = orig_sr, target_sr = target_sr)\n",
        "\n",
        "  # sampling rate after changed\n",
        "  sr = target_sr\n",
        "\n",
        "  if max_length is None:\n",
        "    max_length = librosa.get_duration(y=audio, sr=sr)\n",
        "\n",
        "  # padding: return tensor with padding\n",
        "  audio = padding(audio, sr, max_length)\n",
        "\n",
        "  # framing: return tensor of framing\n",
        "  frames = get_frames(audio, sr)\n",
        "  # to tensor\n",
        "  if not isinstance(frames, torch.Tensor):\n",
        "    frames = torch.from_numpy(frames)\n",
        "\n",
        "\n",
        "  audio = torch.from_numpy(audio)\n",
        "  return audio, frames\n",
        "\n",
        "def get_label_len(df, path_col):\n",
        "  sample_path = df[path_col][0]\n",
        "  audio, sr = torchaudio.load(sample_path)\n",
        "\n",
        "  transformed_audio, frames = transform(df, audio, sr, 16000)\n",
        "  return frames.shape[0]\n",
        "\n",
        "# tạo được kiểu dữ liệu là frames luôn thì tốt\n",
        "\n",
        "def get_label(frames, frame_dur, hop_dur, sr, label_length, path: str, df: pd.DataFrame, num_additional: int = 2):\n",
        "  '''\n",
        "  create label for each waveform (a bunch of frames).\n",
        "  The step is:\n",
        "  - create initial label as a zeros tensor with fix length (the fix length will be decided in main class).\n",
        "  - get all the row with the same path (the path of audio that we want to process)\n",
        "  - go through all the row (interval or segment), if they are labelled as 1, then change all the frame in this interval as 1.\n",
        "  + get the floor frame of start and ceil frame of end (each time can be in more than 1 frame)\n",
        "  + can be label additional frame with num_additional\n",
        "  - mathematics:\n",
        "  + let frame_dur = l, hop_dur = p, find i (time point) in which frames?\n",
        "  + i is in [k * p, k * p + l] as k is the index of frame\n",
        "  + so k * p <= i <= k * p + l\n",
        "  + we want to find k, so floor((i-l)/p) <= k <= ceil((i)/p)\n",
        "\n",
        "  Args:\n",
        "    frames: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    frame_dur: frame duration (in second)\n",
        "    hop_dur: hop duration (in second)\n",
        "    sr: sampling rate, it should be converted to the same in each audio (this is processed in transform())\n",
        "    path: path of audio that will be processed\n",
        "    df: dataframe for annotation\n",
        "    num_additional: additional frame\n",
        "\n",
        "  Returns:\n",
        "  label: label for each frame of waveform, with shape (num_frame, 1)\n",
        "  '''\n",
        "\n",
        "  # create zeros tensor\n",
        "  label = torch.zeros(label_length)\n",
        "  # get all row with processed audio path\n",
        "  df_label = df[df['audio_path'] == path]\n",
        "\n",
        "\n",
        "  # labelling\n",
        "  for i in range(len(df_label)):\n",
        "    mark = df_label.iloc[i]['mark']\n",
        "    if mark == '1':\n",
        "      start = df_label.iloc[i]['start']\n",
        "      end = df_label.iloc[i]['end']\n",
        "\n",
        "      # mathematics is all you need\n",
        "      frame_start = round((start - frame_dur)/hop_dur) - num_additional\n",
        "      frame_end = round(end/hop_dur) + num_additional\n",
        "\n",
        "\n",
        "      # in case out of range\n",
        "      if frame_start < 0:\n",
        "        frame_start = 0\n",
        "\n",
        "      if frame_end > frames.shape[0]:\n",
        "        frame_end = frames.shape[0]\n",
        "\n",
        "      label[frame_start:frame_end] = 1\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  # for better shape\n",
        "  label = label.unsqueeze(-1)\n",
        "  return label\n"
      ],
      "metadata": {
        "id": "SdamL4365fyQ"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "class VADDataset(Dataset):\n",
        "  '''\n",
        "  Dataset for Voice Activity Detection.\n",
        "  * Config file should be integrated for args.\n",
        "\n",
        "  Args:\n",
        "    df: dataframe for annotation\n",
        "    transform: transform for waveform.\n",
        "    df_path: path of audio that will be processed\n",
        "    frame_dur: frame duration (in second)\n",
        "    hop_dur: hop duration (in second)\n",
        "    target_sr: target sampling rate\n",
        "    num_additional: additional frame\n",
        "\n",
        "  Returns:\n",
        "    processed_waveform: processed waveform with padding and resampling (which are the current features of transform() function), may useful for model likes wav2vec2\n",
        "    framed_waveform: waveform after framing, will be converted to np.ndarray so that it can be processed with librosa, with shape (num_frame, frame_size, 1)\n",
        "    label: label for each frame of |waveform, with shape (num_frame, 1)\n",
        "    label_length: length of label (for model output shape)\n",
        "  '''\n",
        "  def __init__(self, df: pd.DataFrame, transform = None):\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "    self.df_path = df['audio_path'].drop_duplicates().reset_index(drop=True)\n",
        "    # save in config\n",
        "    self.frame_dur = 0.025\n",
        "    self.hop_dur = 0.01\n",
        "    self.num_additional = 2\n",
        "    self.target_sr = 16000\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df_path)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.df_path.iloc[idx]\n",
        "    waveform, sr = torchaudio.load(row)\n",
        "\n",
        "    if self.transform:\n",
        "      # transform\n",
        "      processced_waveform, framed_waveform = self.transform(df = self.df, audio = waveform, orig_sr = sr, target_sr = self.target_sr)\n",
        "\n",
        "    # get label\n",
        "    '''\n",
        "    TODO: label_length should be saved in somewhere for model\n",
        "    '''\n",
        "    self.label_length = len(framed_waveform)\n",
        "    label = get_label(\n",
        "        framed_waveform,\n",
        "        label_length=self.label_length,\n",
        "        frame_dur=self.frame_dur,\n",
        "        hop_dur=self.hop_dur,\n",
        "        sr=self.target_sr,\n",
        "        path=row,\n",
        "        df=self.df,\n",
        "        num_additional=self.num_additional)\n",
        "\n",
        "    return processced_waveform, framed_waveform, label"
      ],
      "metadata": {
        "id": "Cw-kfT0cJzSp"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning as L\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "class VADDataModule(L.LightningDataModule):\n",
        "  def __init__(self, df, transform):\n",
        "    super().__init__()\n",
        "\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "    self.batch_size = 32\n",
        "    self.label_length = get_label_len(df, 'audio_path')\n",
        "\n",
        "  def prepare_data(self):\n",
        "    download_data()\n",
        "    self.dataset = VADDataset(df = self.df, transform = self.transform)\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    # TODO: add train/val/test splits\n",
        "\n",
        "    if stage == \"fit\":\n",
        "      self.dataset_train = VADDataset(df = self.df, transform = self.transform)\n",
        "      self.dataset_val = VADDataset(df = self.df, transform = self.transform)\n",
        "\n",
        "    if stage == \"test\":\n",
        "      self.dataset_test = VADDataset(df = self.df, transform = self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.dataset_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.dataset_val, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.dataset_test, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "  def predict_dataloader(self):\n",
        "    return DataLoader(self.dataset_test, batch_size=self.batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "uuBml16r5_2p"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Sd9CmITaY9Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "class Wav2vec2ASR(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.wav2vec2 = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H.get_model()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.wav2vec2(x)\n",
        "\n",
        "class VADMLP(nn.Module):\n",
        "  def __init__(self, in_shape, out_shape):\n",
        "    super().__init__()\n",
        "    self.in_shape = in_shape\n",
        "    self.out_shape = out_shape\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(in_shape, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, out_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n",
        "\n",
        "class VADModel(L.LightningModule):\n",
        "\n",
        "  def __init__(self, out_shape, loss = F.binary_cross_entropy, lr = 1e-3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.backbone = Wav2vec2ASR()\n",
        "    self.in_shape = self.backbone.wav2vec2.aux.in_features\n",
        "    self.out_shape = out_shape\n",
        "\n",
        "    self.mlp = VADMLP(self.in_shape, out_shape)\n",
        "    self.loss = loss\n",
        "    self.lr = lr\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    processed_waveform, framed_waveform, label = batch\n",
        "    features, _ = self.backbone.extract_features(processed_waveform)\n",
        "\n",
        "\n",
        "    output = self.mlp(features[-1])\n",
        "\n",
        "    loss = self.loss(output, label)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "    return optimizer\n",
        ""
      ],
      "metadata": {
        "id": "bFDYgqAwY-l4"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datamodule = VADDataModule(df = df, transform = transform)\n",
        "\n",
        "model = VADModel(out_shape = data_module.label_length)"
      ],
      "metadata": {
        "id": "FXRDpf-tkguG"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning as L\n",
        "\n",
        "trainer = L.Trainer(limit_train_batches=100, max_epochs=1)\n",
        "\n",
        "trainer.fit(model = model, datamodule = datamodule)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0d8ac2dcf8c14765a99dcc35db631137",
            "dce8243e12fa496a942af4ca32990b44",
            "5bc693946faf4477b97ef877475137c5",
            "cf2ef7335f1d402a82ef5a49d3eed278",
            "955dd3d716a249ec9c0d8a23f6aa342b",
            "6f012c616d3a49b79879c6557b4ad938",
            "8f2b3aaf614b4d85932187514a95b0e0",
            "529887cab36b4127abd618c9303f183c",
            "185de26c48b341d7a6a68ac605ccfed3",
            "7e4038faf09644fd9047128569d2292f",
            "9eb871cc22ee44ad91f19f170a906701"
          ]
        },
        "id": "UPQe10CDcgQA",
        "outputId": "2fda7b5a-fe0b-4ad0-ea74-db43e261d1fa"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name     | Type        | Params | Mode \n",
            "-------------------------------------------------\n",
            "0 | backbone | Wav2vec2ASR | 94.4 M | train\n",
            "1 | mlp      | VADMLP      | 832 K  | train\n",
            "-------------------------------------------------\n",
            "95.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "95.2 M    Total params\n",
            "380.905   Total estimated model params size (MB)\n",
            "8         Modules in train mode\n",
            "201       Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d8ac2dcf8c14765a99dcc35db631137"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [1, 192000] at entry 0 and [2, 192000] at entry 6",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-187-838544222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit_train_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         )\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mdataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;31m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;31m# fetcher state so that the batch_idx is correct after restarting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# this will run only when no pre-fetching was done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# the iterator is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_profiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_ITERATOR_RETURN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Sequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consumed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 192000] at entry 0 and [2, 192000] at entry 6"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Wav2vec2ASR()"
      ],
      "metadata": {
        "id": "vZ8jEhwNdXqq"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crZKgQcBlar_",
        "outputId": "ca197258-a927-4bba-a520-77cb7831a799"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Wav2vec2ASR(\n",
              "  (wav2vec2): Wav2Vec2Model(\n",
              "    (feature_extractor): FeatureExtractor(\n",
              "      (conv_layers): ModuleList(\n",
              "        (0): ConvLayerBlock(\n",
              "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
              "        )\n",
              "        (1-4): 4 x ConvLayerBlock(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "        )\n",
              "        (5-6): 2 x ConvLayerBlock(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (encoder): Encoder(\n",
              "      (feature_projection): FeatureProjection(\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (projection): Linear(in_features=512, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (transformer): Transformer(\n",
              "        (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
              "          (conv): ParametrizedConv1d(\n",
              "            768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "            (parametrizations): ModuleDict(\n",
              "              (weight): ParametrizationList(\n",
              "                (0): _WeightNorm()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x EncoderLayer(\n",
              "            (attention): SelfAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (feed_forward): FeedForward(\n",
              "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
              "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (aux): Linear(in_features=768, out_features=29, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7kBWHRoRlbgM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}