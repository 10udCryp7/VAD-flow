{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10udCryp7/VAD-flow/blob/main/notebooks/VAD_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQgT1vcwXYCJ"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q condacolab\n",
        "# import condacolab\n",
        "# condacolab.install()"
      ],
      "metadata": {
        "id": "RtEyFzsDt9l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning textgrid"
      ],
      "metadata": {
        "id": "CueYAbkjCTAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-mVBgPvXb8s"
      },
      "source": [
        "# Annotation Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfG7AXQBMiEe"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lazyrac00n/speech-activity-detection-datasets\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX9zSPDeNpzF"
      },
      "outputs": [],
      "source": [
        "dir = path + '/Data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbMO6woNP9Xs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from textgrid import TextGrid\n",
        "\n",
        "data = []\n",
        "list_dir = []\n",
        "# loop through all files and extract annotation information\n",
        "for dirname, _, filenames in os.walk(f'{dir}/Annotation'):\n",
        "    for filename in filenames:\n",
        "        if not filename.endswith('.TextGrid'):\n",
        "            continue\n",
        "\n",
        "        tg_path = os.path.join(dirname, filename)\n",
        "        tg = TextGrid.fromFile(tg_path)\n",
        "\n",
        "        # loop through all segment in a file, each is saved as a new row with the same name\n",
        "        for interval in tg[0]:\n",
        "            data.append({\n",
        "                'name': filename.replace('.TextGrid', ''),\n",
        "                'mark': interval.mark,\n",
        "                'start': interval.minTime,\n",
        "                'end': interval.maxTime,\n",
        "                'tg_path': tg_path,\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLMt4_EVVHy_"
      },
      "outputs": [],
      "source": [
        "df['audio_path'] = df['tg_path'].str.replace('/Annotation/', '/Audio/', regex=False).str.replace('.TextGrid', '.wav', regex=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQDNzA5NV6yo"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uCEcHBDXjeI"
      },
      "source": [
        "# Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Uccf-F4HK1"
      },
      "outputs": [],
      "source": [
        "# audio framing: Test code for audio framing\n",
        "import librosa\n",
        "import librosa.util\n",
        "\n",
        "audio_sample_dir = df['audio_path'][0]\n",
        "audio, sr = librosa.load(audio_sample_dir, sr=16000)\n",
        "\n",
        "frame_dur = 0.025\n",
        "frame_length = int(frame_dur * sr)\n",
        "\n",
        "hop_dur = 0.01\n",
        "hop_length = int(hop_dur * sr)\n",
        "frames = librosa.util.frame(audio, frame_length=frame_length, hop_length=hop_length)\n",
        "frames = frames.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdk5tDy4xKVE"
      },
      "outputs": [],
      "source": [
        "# duration: see the histogram of duration -> for deciding padding strategy\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "duration_list = []\n",
        "for audio_path in df['audio_path']:\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    duration = librosa.get_duration(y=audio, sr=sr)\n",
        "    duration_list.append(duration)\n",
        "# add duration col\n",
        "df['duration'] = duration_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh0b9D_rFuKo"
      },
      "outputs": [],
      "source": [
        "# histogram\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(duration_list, bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Histogram of Audio Durations\")\n",
        "plt.xlabel(\"Duration (seconds)\")\n",
        "plt.ylabel(\"Number of audio files\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdamL4365fyQ"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "from textgrid import TextGrid\n",
        "import librosa\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def download_data() -> str:\n",
        "    '''\n",
        "    Download dataset from kagglehub: speech-activity-detection-datasets\n",
        "\n",
        "    Returns:\n",
        "        str: path to dataset files\n",
        "\n",
        "    Raises:\n",
        "        Exception: if error occurs\n",
        "    '''\n",
        "    try:\n",
        "      path = kagglehub.dataset_download(\"lazyrac00n/speech-activity-detection-datasets\")\n",
        "    except Exception as e:\n",
        "      print(f'Error: {e}')\n",
        "      return None\n",
        "\n",
        "    return path\n",
        "\n",
        "def build_dataframe(data_dir: str) -> pd.DataFrame:\n",
        "    '''\n",
        "    Build dataframe from dataset files.\n",
        "    The strategy is treat each segment of a audio as a new row in the dataframe.\n",
        "    Use this dataframe by get all row with the same audio name.\n",
        "\n",
        "    Each row in the dataframe has the following columns:\n",
        "    - name: name of the audio file\n",
        "    - mark: label of the segment\n",
        "    - start: start time of the segment\n",
        "    - end: end time of the segment\n",
        "    - tg_path: path to the textgrid file\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): path to dataset files\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: dataframe with columns: name, mark, start, end, tg_path, audio_path, duration\n",
        "    '''\n",
        "\n",
        "    # processing speed may still can be improved\n",
        "    data = []\n",
        "    list_dir = []\n",
        "\n",
        "    # loop through all files for processing\n",
        "    for dirname, _, filenames in os.walk(f'{data_dir}/Annotation'):\n",
        "        for filename in filenames:\n",
        "            # only open TextGrid file\n",
        "            if not filename.endswith('.TextGrid'):\n",
        "                continue\n",
        "\n",
        "            # open TextGrid for processing\n",
        "            tg_path = os.path.join(dirname, filename)\n",
        "            tg = TextGrid.fromFile(tg_path)\n",
        "\n",
        "            # loop through all interval (segment) in a file\n",
        "            for interval in tg[0]:\n",
        "                data.append({\n",
        "                    'name': filename.replace('.TextGrid', ''),\n",
        "                    'mark': interval.mark,\n",
        "                    'start': interval.minTime,\n",
        "                    'end': interval.maxTime,\n",
        "                    'tg_path': tg_path,\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    # create path for easier access\n",
        "    df['audio_path'] = df['tg_path'].str.replace('/Annotation/', '/Audio/', regex=False).str.replace('.TextGrid', '.wav', regex=False)\n",
        "    # create duration for analysis (and may be useful later)\n",
        "    df['duration'] = df['audio_path'].apply(lambda x: librosa.get_duration(filename=x))\n",
        "    return df\n",
        "\n",
        "def padding(audio, sr, max_length):\n",
        "  '''\n",
        "  padding in the end of each audio so that they have the same length.\n",
        "  padding before framing.\n",
        "  maybe can upgrade by using other mode.\n",
        "\n",
        "  Args:\n",
        "    audio: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    sr: sampling rate, it should be converted to the same in each audio (this is processed in transform())\n",
        "    max_length: the fix_length for all audio file (in second)\n",
        "  '''\n",
        "\n",
        "  if not isinstance(audio, np.ndarray):\n",
        "    audio = np.array(audio)\n",
        "  audio = librosa.util.fix_length(audio, size = max_length * sr)\n",
        "  return audio\n",
        "\n",
        "\n",
        "def get_frames(audio, sr, frame_dur = 0.025, hop_dur = 0.01) -> torch.Tensor:\n",
        "  '''\n",
        "  Audio framing (useful for Voice Activity Detection)\n",
        "  Args:\n",
        "    audio: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    sr: sampling rate, it should be converted to the same in each audio (this is processed in transform\n",
        "\n",
        "    frame_dur: frame duration (in second) that used for compute frame_length (used for framing uiwht librosa.util.frame)\n",
        "    hop_dur: hop duration (in second) that used for compute hop_length (used for framing uiwht librosa.util.frame)\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: frames with shape (num_frame, frame_size)\n",
        "  '''\n",
        "  frame_dur = frame_dur\n",
        "  frame_length = int(frame_dur * sr)\n",
        "\n",
        "  hop_dur = hop_dur\n",
        "  hop_length = int(hop_dur * sr)\n",
        "\n",
        "  if not isinstance(audio, np.ndarray):\n",
        "    audio = np.array(audio)\n",
        "\n",
        "  frames = librosa.util.frame(audio, frame_length=frame_length, hop_length=hop_length)\n",
        "\n",
        "  # transpose for easier processing (num_frame, frame_size)\n",
        "  frames = frames.T\n",
        "\n",
        "  # convert to tensor\n",
        "  if not isinstance(frames, torch.Tensor):\n",
        "    frames = torch.from_numpy(frames)\n",
        "\n",
        "  return frames # (num_frame, frame_size)\n",
        "\n",
        "def resample(audio, orig_sr, target_sr):\n",
        "  '''\n",
        "  resampling so that all audio file have the same sampling rate\n",
        "\n",
        "  Args:\n",
        "    audio: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    orig_sr: original sampling rate\n",
        "    target_sr: target sampling rate\n",
        "\n",
        "  Returns:\n",
        "    np.ndarray: resampled audio\n",
        "  '''\n",
        "  if not isinstance(audio, np.ndarray):\n",
        "    audio = np.array(audio)\n",
        "\n",
        "  audio = librosa.resample(y = audio, orig_sr = orig_sr, target_sr = target_sr)\n",
        "  return audio\n",
        "\n",
        "def transform(df, audio, orig_sr, target_sr):\n",
        "  '''\n",
        "  TODO: add mel-spectrogram option\n",
        "\n",
        "\n",
        "  transform for training, with three steps:\n",
        "  1. resampling\n",
        "  2. padding\n",
        "  3. framing\n",
        "\n",
        "  Args:\n",
        "    audio: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    orig_sr: original sampling rate\n",
        "    target_sr: target sampling rate\n",
        "\n",
        "  Returns:\n",
        "    audio: resampled audio with padding and resampling, with shape (waveform, )\n",
        "    frames: waveform after framing, will be converted to np.ndarray so that it can be processed with librosa, with shape (num_frame, frame_size, 1)\n",
        "  '''\n",
        "  # get max length (plus 0.5) for better number rounding\n",
        "\n",
        "  max_length = round(max(df['duration']) + 0.5)\n",
        "  # resampling: return np.ndarray\n",
        "  audio = resample(audio = audio, orig_sr = orig_sr, target_sr = target_sr)\n",
        "\n",
        "  # sampling rate after changed\n",
        "  sr = target_sr\n",
        "\n",
        "  if max_length is None:\n",
        "    max_length = librosa.get_duration(y=audio, sr=sr)\n",
        "\n",
        "  # padding: return tensor with padding\n",
        "  audio = padding(audio, sr, max_length)\n",
        "\n",
        "  # framing: return tensor of framing\n",
        "  frames = get_frames(audio, sr)\n",
        "  # to tensor\n",
        "  if not isinstance(frames, torch.Tensor):\n",
        "    frames = torch.from_numpy(frames)\n",
        "\n",
        "\n",
        "  audio = torch.from_numpy(audio).squeeze(0) # (waveform_len, )\n",
        "  return audio, frames\n",
        "\n",
        "def get_label_len(df, path_col):\n",
        "  sample_path = df[path_col][0]\n",
        "  audio, sr = torchaudio.load(sample_path)\n",
        "\n",
        "  transformed_audio, frames = transform(df, audio, sr, 16000)\n",
        "  return frames.shape[0]\n",
        "\n",
        "# tạo được kiểu dữ liệu là frames luôn thì tốt\n",
        "\n",
        "def get_label(frames, frame_dur, hop_dur, sr, label_length, path: str, df: pd.DataFrame, num_additional: int = 2):\n",
        "  '''\n",
        "  create label for each waveform (a bunch of frames).\n",
        "  The step is:\n",
        "  - create initial label as a zeros tensor with fix length (the fix length will be decided in main class).\n",
        "  - get all the row with the same path (the path of audio that we want to process)\n",
        "  - go through all the row (interval or segment), if they are labelled as 1, then change all the frame in this interval as 1.\n",
        "  + get the floor frame of start and ceil frame of end (each time can be in more than 1 frame)\n",
        "  + can be label additional frame with num_additional\n",
        "  - mathematics:\n",
        "  + let frame_dur = l, hop_dur = p, find i (time point) in which frames?\n",
        "  + i is in [k * p, k * p + l] as k is the index of frame\n",
        "  + so k * p <= i <= k * p + l\n",
        "  + we want to find k, so floor((i-l)/p) <= k <= ceil((i)/p)\n",
        "\n",
        "  Args:\n",
        "    frames: waveform, will be converted to np.ndarray so that it can be processed with librosa\n",
        "    frame_dur: frame duration (in second)\n",
        "    hop_dur: hop duration (in second)\n",
        "    sr: sampling rate, it should be converted to the same in each audio (this is processed in transform())\n",
        "    path: path of audio that will be processed\n",
        "    df: dataframe for annotation\n",
        "    num_additional: additional frame\n",
        "\n",
        "  Returns:\n",
        "  label: label for each frame of waveform, with shape (num_frame, 1)\n",
        "  '''\n",
        "\n",
        "  # create zeros tensor\n",
        "  label = torch.zeros(label_length)\n",
        "  # get all row with processed audio path\n",
        "  df_label = df[df['audio_path'] == path]\n",
        "\n",
        "\n",
        "  # labelling\n",
        "  for i in range(len(df_label)):\n",
        "    mark = df_label.iloc[i]['mark']\n",
        "    if mark == '1':\n",
        "      start = df_label.iloc[i]['start']\n",
        "      end = df_label.iloc[i]['end']\n",
        "\n",
        "      # mathematics is all you need\n",
        "      frame_start = round((start - frame_dur)/hop_dur) - num_additional\n",
        "      frame_end = round(end/hop_dur) + num_additional\n",
        "\n",
        "\n",
        "      # in case out of range\n",
        "      if frame_start < 0:\n",
        "        frame_start = 0\n",
        "\n",
        "      if frame_end > frames.shape[0]:\n",
        "        frame_end = frames.shape[0]\n",
        "\n",
        "      label[frame_start:frame_end] = 1\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  # for better shape\n",
        "  label = label.unsqueeze(-1)\n",
        "  return label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw-kfT0cJzSp"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "class VADDataset(Dataset):\n",
        "  '''\n",
        "  Dataset for Voice Activity Detection.\n",
        "  * Config file should be integrated for args.\n",
        "\n",
        "  Args:\n",
        "    df: dataframe for annotation\n",
        "    transform: transform for waveform.\n",
        "    df_path: path of audio that will be processed\n",
        "    frame_dur: frame duration (in second)\n",
        "    hop_dur: hop duration (in second)\n",
        "    target_sr: target sampling rate\n",
        "    num_additional: additional frame\n",
        "\n",
        "  Returns:\n",
        "    processed_waveform: processed waveform with padding and resampling (which are the current features of transform() function), may useful for model likes wav2vec2\n",
        "    framed_waveform: waveform after framing, will be converted to np.ndarray so that it can be processed with librosa, with shape (num_frame, frame_size, 1)\n",
        "    label: label for each frame of |waveform, with shape (num_frame, 1)\n",
        "    path: path of audio that will be processed\n",
        "  '''\n",
        "  def __init__(self, df: pd.DataFrame, transform = None):\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "    self.df_path = df['audio_path'].drop_duplicates().reset_index(drop=True)\n",
        "    # save in config\n",
        "    self.frame_dur = 0.025\n",
        "    self.hop_dur = 0.01\n",
        "    self.num_additional = 2\n",
        "    self.target_sr = 16000\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df_path)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path = self.df_path.iloc[idx]\n",
        "    waveform, sr = torchaudio.load(path)\n",
        "\n",
        "    if waveform.shape[0] > 1:\n",
        "      waveform = waveform.mean(dim=0, keepdim=True)  # (1, time)\n",
        "\n",
        "    if self.transform:\n",
        "      # transform\n",
        "      processced_waveform, framed_waveform = self.transform(df = self.df, audio = waveform, orig_sr = sr, target_sr = self.target_sr)\n",
        "    # get label\n",
        "    label = get_label(\n",
        "        framed_waveform,\n",
        "        label_length=len(framed_waveform),  # Calculate label length dynamically\n",
        "        frame_dur=self.frame_dur,\n",
        "        hop_dur=self.hop_dur,\n",
        "        sr=self.target_sr,\n",
        "        path=path,\n",
        "        df=self.df,\n",
        "        num_additional=self.num_additional)\n",
        "\n",
        "    return processced_waveform, framed_waveform, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuBml16r5_2p"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "class VADDataModule(L.LightningDataModule):\n",
        "  def __init__(self, df, transform):\n",
        "    super().__init__()\n",
        "\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "    self.batch_size = 1\n",
        "    self.label_length = get_label_len(df, 'audio_path') # Removed static label_length calculation\n",
        "\n",
        "  def prepare_data(self):\n",
        "    download_data()\n",
        "    # self.dataset = VADDataset(df = self.df, transform = self.transform) # Dataset is created in setup\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    # TODO: add train/val/test splits\n",
        "\n",
        "    if stage == \"fit\" or stage is None:\n",
        "      self.dataset_train = VADDataset(df = self.df, transform = self.transform)\n",
        "      self.dataset_val = VADDataset(df = self.df, transform = self.transform)\n",
        "\n",
        "    if stage == \"test\" or stage is None:\n",
        "      self.dataset_test = VADDataset(df = self.df, transform = self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.dataset_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.dataset_val, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.dataset_test, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "  def predict_dataloader(self):\n",
        "    return DataLoader(self.dataset_test, batch_size=self.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd9CmITaY9Yo"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFDYgqAwY-l4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class Wav2vec2ASR(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.wav2vec2 = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H.get_model()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.wav2vec2(x)\n",
        "\n",
        "  def get_model(self):\n",
        "    return self.wav2vec2\n",
        "\n",
        "class VADMLP(nn.Module):\n",
        "  def __init__(self, time_shape, feature_shape, out_shape):\n",
        "    super().__init__()\n",
        "    self.time_shape = time_shape\n",
        "    self.feature_shape = feature_shape\n",
        "    self.out_shape = out_shape\n",
        "\n",
        "    self.LSTM = nn.Sequential(\n",
        "        nn.LSTM(self.feature_shape, hidden_size = 512, batch_first=True, bidirectional = True),\n",
        "    )\n",
        "\n",
        "    self.linear_time = nn.Sequential(\n",
        "        nn.Linear(self.time_shape, 1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(1024, out_shape),\n",
        "\n",
        "    )\n",
        "    self.linear_feature = nn.Sequential(\n",
        "        nn.Linear(512 * 2, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _ = self.LSTM(x)\n",
        "    x = torch.permute(x, (0, 2, 1)) # torch.Size([32, 1024, 599]) because of bidirectional\n",
        "\n",
        "    x = self.linear_time(x)\n",
        "    x = torch.permute(x, (0, 2, 1)) # torch.Size([32, 1198, 1024])\n",
        "\n",
        "    x = self.linear_feature(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class VADModel(L.LightningModule):\n",
        "\n",
        "  def __init__(self, out_shape, loss = F.binary_cross_entropy, lr = 1e-3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.backbone = Wav2vec2ASR()\n",
        "    for param in self.backbone.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    self.feature_shape = self.backbone.get_model().aux.in_features\n",
        "    self.time_shape = 599\n",
        "    self.out_shape = out_shape\n",
        "\n",
        "    self.mlp = VADMLP(self.time_shape, self.feature_shape, self.out_shape)\n",
        "    self.loss = loss\n",
        "    self.lr = lr\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    processed_waveform, framed_waveform, label = batch\n",
        "    features, _ = self.backbone.get_model().extract_features(processed_waveform)\n",
        "\n",
        "    output = self.mlp(features[-1])\n",
        "\n",
        "    loss = self.loss(output, label)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "    return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXRDpf-tkguG"
      },
      "outputs": [],
      "source": [
        "datamodule = VADDataModule(df = df, transform = transform)\n",
        "\n",
        "model = VADModel(out_shape = datamodule.label_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPQe10CDcgQA"
      },
      "outputs": [],
      "source": [
        "trainer = L.Trainer(max_epochs = 10)\n",
        "\n",
        "trainer.fit(model = model, datamodule = datamodule)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyNDyTI2No3Qef0HSYFaIBxJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}